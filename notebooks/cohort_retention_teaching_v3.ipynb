{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "v3-000",
   "source": "# Cohort Retention Teaching Notebook v3 (Explain-First)\n\nThis version is optimized for teaching clarity: start with grains and sanity, then definitions, then charts, then an exercise.\n\n**Export rule used in this notebook:** code is visible and commented for learning.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "v3-001",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup: load only required files and fail fast if anything is missing.\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "REPO_ROOT = None\n",
    "for root in [cwd] + list(cwd.parents):\n",
    "    if (root / 'data_processed').exists() and (root / 'docs').exists():\n",
    "        REPO_ROOT = root\n",
    "        break\n",
    "if REPO_ROOT is None:\n",
    "    raise FileNotFoundError('Could not locate repo root with data_processed/ and docs/.')\n",
    "\n",
    "src_root = REPO_ROOT / 'src'\n",
    "if str(src_root) not in sys.path:\n",
    "    sys.path.insert(0, str(src_root))\n",
    "\n",
    "from retention.policies import HORIZON_H, MIN_COHORT_N, OBSERVED_ONLY, RIGHT_CENSOR_MODE\n",
    "\n",
    "DP = REPO_ROOT / 'data_processed'\n",
    "\n",
    "required = [\n",
    "    DP / 'scope_receipts.json',\n",
    "    DP / 'order_lines.csv',\n",
    "    DP / 'orders.csv',\n",
    "    DP / 'customers.csv',\n",
    "    DP / 'customer_month_activity.csv',\n",
    "    DP / 'chart1_logo_retention_heatmap.csv',\n",
    "    DP / 'chart2_net_proxy_heatmap.csv',\n",
    "    DP / 'chart2_family_scatter.csv',\n",
    "    DP / 'chart3_m2_by_family.csv',\n",
    "    DP / 'gate_a.json',\n",
    "    DP / 'confound_m2_family_all_vs_retail.csv',\n",
    "]\n",
    "missing = [str(p.relative_to(REPO_ROOT)) for p in required if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError('Missing required artifacts: ' + ', '.join(missing))\n",
    "\n",
    "scope = json.loads((DP / 'scope_receipts.json').read_text(encoding='ascii'))\n",
    "gate_a = json.loads((DP / 'gate_a.json').read_text(encoding='ascii'))\n",
    "\n",
    "# Load core tables.\n",
    "order_lines = pd.read_csv(DP / 'order_lines.csv')\n",
    "orders = pd.read_csv(DP / 'orders.csv')\n",
    "customers = pd.read_csv(DP / 'customers.csv')\n",
    "cma = pd.read_csv(DP / 'customer_month_activity.csv')\n",
    "chart1 = pd.read_csv(DP / 'chart1_logo_retention_heatmap.csv')\n",
    "chart2 = pd.read_csv(DP / 'chart2_net_proxy_heatmap.csv')\n",
    "chart2_scatter = pd.read_csv(DP / 'chart2_family_scatter.csv')\n",
    "chart3 = pd.read_csv(DP / 'chart3_m2_by_family.csv')\n",
    "confound = pd.read_csv(DP / 'confound_m2_family_all_vs_retail.csv')\n",
    "\n",
    "print('Loaded all required inputs from', DP)\n",
    "print(f'Policies: H={HORIZON_H}, MIN_COHORT_N={MIN_COHORT_N}, OBSERVED_ONLY={OBSERVED_ONLY}, RIGHT_CENSOR_MODE={RIGHT_CENSOR_MODE}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "v3-002",
   "source": "## 1) Grain Map (raw -> modeled tables)\nThink of this as the data staircase. Every step should have expected grain and row logic.\n\n- Raw Excel rows: original invoice-line records from one or more sheets\n- `order_lines.csv`: normalized line-item grain (should closely reconcile to raw rows)\n- `orders.csv`: one row per order_id\n- `customers.csv`: one row per cohort-universe customer\n- `customer_month_activity.csv`: one row per customer per month 0..6 (exactly 7 rows/customer)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stop & Answer\n1. What does the grain map protect you from?  \nAnswer: It prevents mixing incompatible row levels (line, order, customer, customer-month).\n\n2. If `customers.csv` has fewer rows than `orders.csv`, is that expected?  \nAnswer: Yes. Customers are unique people in the cohort universe; orders can be many per customer."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "v3-003",
   "execution_count": null,
   "outputs": [],
   "source": "# Grain map table with row counts.\ngrain_map = pd.DataFrame([\n    {'stage': 'raw_sum_rows (from scope receipts)', 'rows': int(scope['raw_sum_rows'])},\n    {'stage': 'order_lines.csv', 'rows': int(len(order_lines))},\n    {'stage': 'orders.csv', 'rows': int(len(orders))},\n    {'stage': 'customers.csv', 'rows': int(len(customers))},\n    {'stage': 'customer_month_activity.csv', 'rows': int(len(cma))},\n])\nprint(grain_map.to_string(index=False))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What pattern should you look for? (Heatmaps)\n- Vertical decay: does retention/value fade as months_since_first increases?\n- Cohort shifts: do newer cohort rows look better/worse than older rows at the same month index?\n- Anomalies: sudden spikes/drops can indicate promo periods, returns waves, or mapping/credit issues.\n- White/blank = missing/suppressed, not zero.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### You do -> Then we show: File map for joins\nFirst try to describe each file's grain and join path yourself. Then run the cell to confirm."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# File map: grain, primary key, and joins used in the teaching flow.\nfile_map = pd.DataFrame([\n    {'file': 'scope_receipts.json', 'grain': 'workbook-level receipt', 'primary_key': 'n/a', 'joins_used': 'none'},\n    {'file': 'order_lines.csv', 'grain': 'order line', 'primary_key': 'order_id + sku + order_ts (practical line grain)', 'joins_used': 'roll up to orders by order_id'},\n    {'file': 'orders.csv', 'grain': 'order', 'primary_key': 'order_id', 'joins_used': 'join to customers on customer_id; aggregate to months'},\n    {'file': 'customers.csv', 'grain': 'cohort customer', 'primary_key': 'customer_id', 'joins_used': 'left join onto full month grid'},\n    {'file': 'customer_month_activity.csv', 'grain': 'customer-month (0..6)', 'primary_key': 'customer_id + months_since_first', 'joins_used': 'source for retention metrics and chart tables'},\n])\nprint(file_map.to_string(index=False))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H=6 full grid = 7 rows per customer (0..6)\n",
    "Every cohort-universe customer gets exactly one row for each `months_since_first` in `0..6`.\n",
    "That means **7 rows per customer**, even if a later month has no observed purchases.\n",
    "This guarantees structural comparability before any censoring/suppression logic in chart tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Micro-Lab 1: Build the grid from scratch (for 5 customers)\n**You do:** Rebuild a 0..6 month grid for 5 cohort customers using `customers.csv` + `orders.csv`, then verify core cohort invariants."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# CA-42 micro-lab: rebuild 7-row customer grid for five customers and assert core invariants.\nsample_customers = (\n    customers['customer_id']\n    .astype(str)\n    .drop_duplicates()\n    .sort_values(kind='stable')\n    .head(5)\n    .tolist()\n)\ncust5 = customers[customers['customer_id'].astype(str).isin(sample_customers)][['customer_id', 'cohort_month']].copy()\ncust5['customer_id'] = cust5['customer_id'].astype(str)\ncust5['cohort_period'] = pd.PeriodIndex(cust5['cohort_month'], freq='M')\nmonths = pd.DataFrame({'months_since_first': list(range(7))})\ngrid = cust5.assign(_k=1).merge(months.assign(_k=1), on='_k', how='inner').drop(columns='_k')\ngrid['activity_period'] = grid['cohort_period'] + grid['months_since_first']\ngrid['activity_month'] = grid['activity_period'].astype(str)\n\nvalid_orders = orders[(orders['is_valid_purchase'] == 1)].copy()\nvalid_orders['customer_id'] = valid_orders['customer_id'].astype(str)\nvalid_orders = valid_orders[valid_orders['customer_id'].isin(sample_customers)]\nvalid_orders['activity_month'] = pd.to_datetime(valid_orders['order_ts'], errors='coerce').dt.to_period('M').astype(str)\nvalid_counts = (\n    valid_orders.groupby(['customer_id', 'activity_month'], as_index=False)\n    .agg(orders_count_valid=('order_id', 'nunique'))\n)\n\nlab1 = grid.merge(valid_counts, on=['customer_id', 'activity_month'], how='left')\nlab1['orders_count_valid'] = lab1['orders_count_valid'].fillna(0).astype(int)\nlab1['is_retained_logo'] = (lab1['orders_count_valid'] > 0).astype(int)\n\npass_rows = lab1.groupby('customer_id').size().eq(7).all()\nmonths_ok = (\n    lab1.sort_values(['customer_id', 'months_since_first'], kind='stable')\n    .groupby('customer_id')['months_since_first']\n    .apply(list)\n    .apply(lambda vals: vals == list(range(7)))\n    .all()\n)\nmonth0_ok = (\n    lab1[lab1['months_since_first'] == 0]\n    .groupby('customer_id')['is_retained_logo']\n    .max()\n    .eq(1)\n    .all()\n)\n\nprint(('PASS' if pass_rows else 'FAIL') + ' grid_rows_per_customer')\nprint(('PASS' if months_ok else 'FAIL') + ' months_since_first_0_to_6')\nprint(('PASS' if month0_ok else 'FAIL') + ' month0_logo_retention')\n\ncols = ['customer_id', 'cohort_month', 'activity_month', 'months_since_first', 'orders_count_valid', 'is_retained_logo']\nprint(lab1[cols].sort_values(['customer_id', 'months_since_first'], kind='stable').to_string(index=False))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Expected output**\n\n`PASS grid_rows_per_customer`\n\n`PASS months_since_first_0_to_6`\n\n`PASS month0_logo_retention`\n\n...followed by a 35-row table (5 customers x 7 months)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Plain-English: This lab proves the cohort grid shape is deterministic (7 rows/customer) and that month 0 retention is structural for cohort-universe customers. If any line fails, cohort math is broken before chart interpretation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "v3-004",
   "source": "## 2) Sanity Checks (this prevents false stories)\n### Check A: raw_sum_rows vs order_lines_rows\nIf this is not close, your downstream metrics can be untrustworthy.\n\n### Check B: join explosion detection\nJoin explosion happens when a key is not unique and a join multiplies rows unexpectedly.\nRule used here: processed line rows > raw_sum_rows * 1.05 => **explosion suspected**.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stop & Answer\n1. What would a join explosion look like in receipts?  \nAnswer: `order_lines_rows` materially above `raw_sum_rows` (here threshold is >5%).\n\n2. Why is month0 retention ~100% by definition?  \nAnswer: Cohort customers are defined by having a valid first purchase, so month 0 must include activity."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "v3-005",
   "execution_count": null,
   "outputs": [],
   "source": "# Sanity checks from receipts + modeled tables.\nraw_sum = int(scope['raw_sum_rows'])\nline_rows = int(len(order_lines))\ndelta_pct = ((line_rows - raw_sum) / raw_sum) * 100 if raw_sum else 0.0\n\nif line_rows > raw_sum * 1.05:\n    sanity_flag = 'JOIN_EXPLOSION_SUSPECTED'\nelif line_rows < raw_sum * 0.95:\n    sanity_flag = 'UNDERCOUNT'\nelse:\n    sanity_flag = 'RECONCILED'\n\nrows_per_customer = cma.groupby('customer_id').size()\nmonth0 = cma[cma['months_since_first'] == 0]\n\nprint(f'raw_sum_rows={raw_sum}')\nprint(f'order_lines_rows={line_rows}')\nprint(f'delta_pct={delta_pct:.2f}%')\nprint(f'sanity_flag={sanity_flag}')\nprint(f'full_grid_7_rows_per_customer={(rows_per_customer == 7).all()}')\nprint(f'month0_retention_100pct={(month0[\"is_retained_logo\"] == 1).all()}')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### You do -> Then we show: Reconciliation assert exercise\nExercise: compute `raw_sum_rows` and `order_lines` rows, then assert they are within +/-1%."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Exact exercise code: fail fast if line-item scope is not reconciled.\nraw_sum_rows = int(scope['raw_sum_rows'])\norder_lines_rows = int(len(order_lines))\ndelta_ratio = abs(order_lines_rows - raw_sum_rows) / raw_sum_rows if raw_sum_rows else 0.0\nassert delta_ratio <= 0.01, (\n    f'Reconciliation failed: raw_sum_rows={raw_sum_rows}, order_lines_rows={order_lines_rows}, delta_ratio={delta_ratio:.4%}'\n)\nprint(f'PASS raw_sum_rows={raw_sum_rows} order_lines_rows={order_lines_rows} delta_ratio={delta_ratio:.4%}')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Expected output pattern:\n`PASS raw_sum_rows=1067371 order_lines_rows=1067371 delta_ratio=0.0000%`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Micro-Lab 2: Sanity + Scope Reconciliation\n**You do:** Load `scope_receipts.json`, print scope fields, and assert processed line rows reconcile to raw rows within +/-1%."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# CA-43 micro-lab: scope and reconciliation check from receipts.\nscope_lab = json.loads((DP / 'scope_receipts.json').read_text(encoding='ascii'))\nraw_sum_rows = int(scope_lab['raw_sum_rows'])\nprocessed_order_lines_rows = scope_lab['processed_order_lines_rows']\nprocessed_order_lines_rows = int(processed_order_lines_rows) if processed_order_lines_rows is not None else 0\ndelta_ratio = abs(processed_order_lines_rows - raw_sum_rows) / raw_sum_rows if raw_sum_rows else 0.0\nrecon_ok = delta_ratio <= 0.01\n\nprint('sheets_detected=', scope_lab['sheets_detected'])\nprint('sheet_rows=', scope_lab['sheet_rows'])\nprint('raw_sum_rows=', raw_sum_rows)\nprint('processed_order_lines_rows=', processed_order_lines_rows)\nprint('reconcile_status=', scope_lab['reconcile_status'])\nprint((('PASS' if recon_ok else 'FAIL') + f' scope_reconciliation_within_1pct delta_ratio={delta_ratio:.4%}'))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Expected output**\n\n`reconcile_status= OK_BOTH_SHEETS`\n\n`PASS scope_reconciliation_within_1pct delta_ratio=0.0000%`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Plain-English: We ingested both sheets because the workbook has two yearly tabs and receipts explicitly include both in `sheet_rows` and `raw_sum_rows`. Line-item sanity confirms processed `order_lines` reconciles to raw rows within +/-1%, so join explosion risk is not indicated."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "v3-006",
   "source": "## 3) Frozen Definitions (v1.2)\nThese are fixed project definitions and should be stated consistently:\n\n- **valid purchase**: `(order_gross > 0) & (is_cancel_invoice == 0)`\n- **credit-like**: `is_cancel_invoice OR (order_net_proxy < 0)`\n- **net retention proxy** (cohort month t):\n  `sum(net_revenue_proxy_total at t) / sum(gross_revenue_valid at month 0)`\n- **denominator choice**: exclude cohorts where month-0 denominator is zero\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stop & Answer\n1. Why do we separate `order_gross` and `order_net_proxy`?  \nAnswer: Gross defines valid purchases cleanly; net proxy captures credits/refunds for value tracking.\n\n2. Why exclude cohorts with month-0 denominator = 0 from Chart 2?  \nAnswer: Division would be undefined or unstable, so those cohorts are explicitly ineligible."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "v3-007",
   "execution_count": null,
   "outputs": [],
   "source": "# Show real receipts for Gate A and denominator guard context.\nbaseline = (\n    cma[cma['months_since_first'] == 0]\n    .groupby('cohort_month', as_index=False)\n    .agg(baseline_gross=('gross_revenue_valid', 'sum'))\n)\neligible = int((baseline['baseline_gross'] > 0).sum())\nexcluded = int((baseline['baseline_gross'] == 0).sum())\n\nprint(f\"Gate A pct valid purchases with non-positive net: {gate_a['gate_a_pct_valid_nonpositive_net']:.4f}%\")\nprint(f\"Gate A trigger fired: {gate_a['trigger_fired']}\")\nprint(f\"Chart2 denominator guard cohorts: eligible={eligible}, excluded={excluded}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### You do -> Then we show: Credit-like truth table\nPredict outcomes first, then verify with the 4-row truth table."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Truth table examples for is_credit_like = is_cancel_invoice OR (order_net_proxy < 0).\ntruth = pd.DataFrame([\n    {'example': 'A', 'is_cancel_invoice': 0, 'order_net_proxy':  12.50},\n    {'example': 'B', 'is_cancel_invoice': 0, 'order_net_proxy':  -3.20},\n    {'example': 'C', 'is_cancel_invoice': 1, 'order_net_proxy':  14.00},\n    {'example': 'D', 'is_cancel_invoice': 1, 'order_net_proxy':  -8.00},\n])\ntruth['net_proxy_lt_zero'] = truth['order_net_proxy'] < 0\ntruth['is_credit_like'] = (truth['is_cancel_invoice'] == 1) | truth['net_proxy_lt_zero']\nprint(truth[['example', 'is_cancel_invoice', 'order_net_proxy', 'net_proxy_lt_zero', 'is_credit_like']].to_string(index=False))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Micro-Lab 3: Net proxy retention (manual)\n**You do:** For one cohort (Chart 2 cohort grain), manually rebuild the net proxy ratio 0..6 and validate it against `chart2_net_proxy_curves.csv`."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# CA-44 micro-lab: manual Chart 2 reconstruction for one cohort and one family grain (ALL_FAMILIES).\ncohort_choice = str(chart2['cohort_month'].dropna().sort_values(kind='stable').iloc[0])\nfamily_choice = 'ALL_FAMILIES'\nmax_observed_month = pd.to_datetime(scope['raw_max_date'], errors='coerce').to_period('M')\n\ncma_one = cma[cma['cohort_month'] == cohort_choice].copy()\ngross_m0 = float(cma_one.loc[cma_one['months_since_first'] == 0, 'gross_revenue_valid'].sum())\nnet_curve = (\n    cma_one.groupby('months_since_first', as_index=False)\n    .agg(net_proxy=('net_revenue_proxy_total', 'sum'))\n    .query('months_since_first >= 0 and months_since_first <= 6')\n    .sort_values('months_since_first', kind='stable')\n)\ncohort_period = pd.Period(cohort_choice, freq='M')\nnet_curve['activity_period'] = cohort_period + net_curve['months_since_first'].astype(int)\nnet_curve['is_observed'] = net_curve['activity_period'] <= max_observed_month\nnet_curve['gross_m0'] = gross_m0\nnet_curve['ratio'] = net_curve['net_proxy'] / gross_m0 if gross_m0 > 0 else pd.NA\n\n# Effective n for this point = customers with any activity signal in that month.\nnet_curve['n_customers'] = 0\nfor i in range(len(net_curve)):\n    m = int(net_curve.loc[i, 'months_since_first'])\n    month_rows = cma_one[cma_one['months_since_first'] == m].copy()\n    has_any = (month_rows['orders_count_valid'] > 0) | (month_rows['net_revenue_proxy_total'] != 0)\n    net_curve.loc[i, 'n_customers'] = int(month_rows.loc[has_any, 'customer_id'].nunique())\n\nnet_curve.loc[~net_curve['is_observed'], 'ratio'] = pd.NA\nnet_curve.loc[net_curve['n_customers'] < 50, 'ratio'] = pd.NA\n\ncmp = chart2[chart2['cohort_month'] == cohort_choice][['months_since_first', 'net_retention_proxy']].copy()\ncmp['months_since_first'] = cmp['months_since_first'].astype(int)\ncheck = net_curve.merge(cmp, on='months_since_first', how='left')\ncheck['abs_diff'] = (check['ratio'] - check['net_retention_proxy']).abs()\nmax_abs_diff = float(check['abs_diff'].fillna(0).max()) if len(check) else 0.0\n\ndenom_ok = gross_m0 > 0\nmatch_ok = max_abs_diff <= 1e-9\n\nprint(f'selected_cohort_month={cohort_choice}')\nprint(f'selected_family={family_choice}')\nprint(f'max_observed_month={max_observed_month}')\nprint(('PASS' if denom_ok else 'FAIL') + ' denominator_positive')\nprint(('PASS' if match_ok else 'FAIL') + f' chart2_ratio_match_within_rounding max_abs_diff={max_abs_diff:.12f}')\nprint(check[['months_since_first', 'gross_m0', 'net_proxy', 'n_customers', 'is_observed', 'ratio']].to_string(index=False))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Expected output**\n\n`PASS denominator_positive`\n\n`PASS chart2_ratio_match_within_rounding ...`\n\n...plus a 7-row table with `months_since_first`, `gross_m0`, `net_proxy`, `ratio`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Plain-English (why denominator is fixed):\n- Month 0 gross valid revenue is the common baseline, so each later month is comparable on the same scale.\n- Keeping denominator fixed isolates value decay/recovery in the numerator (`net_revenue_proxy_total`).\n- Right-censoring sets unobserved future months to missing, which avoids treating end-of-data as real zero value.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right-censoring: why later cohorts have missing months\n",
    "When the dataset ends (raw_max_date), some future cohort-month combinations are **unobservable**.\n",
    "For those cells, we keep values as missing (`NaN`), not zero, so we do not fake a retention collapse.\n",
    "\n",
    "Explicit note: **\"2011-06 gets 'darker' later\" is only interpretable for observable months; any post-cutoff darkness/blankness would be a chart artifact if treated as zero.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example: near-end cohort observability and censoring behavior.\n",
    "cohort_example = '2011-06'\n",
    "raw_max = pd.to_datetime(scope['raw_max_date'], errors='coerce')\n",
    "max_obs = raw_max.to_period('M')\n",
    "\n",
    "ex = chart2[chart2['cohort_month'] == cohort_example][['cohort_month', 'months_since_first', 'net_retention_proxy', 'n_customers']].copy()\n",
    "if ex.empty:\n",
    "    ex = chart2[chart2['cohort_month'] != 'ALL_WEIGHTED'][['cohort_month', 'months_since_first', 'net_retention_proxy', 'n_customers']].copy()\n",
    "    cohort_example = str(ex['cohort_month'].astype(str).sort_values(kind='stable').iloc[-1])\n",
    "    ex = ex[ex['cohort_month'] == cohort_example].copy()\n",
    "\n",
    "ex['cohort_period'] = pd.PeriodIndex(ex['cohort_month'].astype(str), freq='M')\n",
    "ex['activity_period'] = ex['cohort_period'] + ex['months_since_first'].astype(int)\n",
    "ex['activity_month'] = ex['activity_period'].astype(str)\n",
    "ex['is_observable'] = ex['activity_period'] <= max_obs\n",
    "ex['is_missing_value'] = ex['net_retention_proxy'].isna()\n",
    "\n",
    "print(f'cohort_example={cohort_example}, max_observed_month={max_obs}')\n",
    "print(ex[['months_since_first', 'activity_month', 'is_observable', 'n_customers', 'net_retention_proxy', 'is_missing_value']].to_string(index=False))\n",
    "\n",
    "print('Why missing must be missing (not zero): unobservable months are outside dataset scope, so zero would be a fabricated outcome.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini exercise:\n",
    "1. Pick one `customer_id`.\n",
    "2. Print their 7-row grid (`months_since_first` 0..6).\n",
    "3. Explain which rows are censored (if `activity_month` is after max observed month) and why those rows are *missing in chart tables*, not zero behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "v3-008",
   "source": [
    "## 4) The 3 Charts (question each chart answers)\n",
    "1. **Heatmap (logo retention)**: \"How does repeat behavior decay across cohorts over months 0..H?\"\n",
    "2. **Family scatter (M2)**: \"Which first-order families combine weak repeat (logo) and weak value after credits (net proxy)?\"\n",
    "3. **M2 by first_product_family**: \"Which first families are strongest/weakest at Month 2 and how big are samples?\"\n",
    "\n",
    "Right-censor rule: months after max_observed_month are unobserved and shown as missing, not zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Why we do not show 3 random cohort curves (and use a heatmap instead)\n- Cherry-pick risk: picking 3 cohorts can accidentally tell the story you want instead of the story the data supports.\n- Censoring distortion: late cohorts hit end-of-data, which can look like a crash unless missing is shown explicitly.\n- Decision mismatch: the exec question is overall pattern and where to act, not one-off cohort anecdotes.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Chart 1 Caption Prompt\n**Write caption here:** _[Your one-sentence caption]_\n\n<details><summary>Now compare to answer key</summary>\n\nOpen `docs/TEACHING_ANSWER_KEY.md` -> **Chart 1**.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Chart 2 Caption Prompt\n**Write caption here:** _[Your one-sentence caption]_\n\n<details><summary>Now compare to answer key</summary>\n\nOpen `docs/TEACHING_ANSWER_KEY.md` -> **Chart 2** and map to memo section `## Plays` (refund drag / returns mitigation).\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Chart 3 Caption Prompt\n**Write caption here:** _[Your one-sentence caption]_\n\n<details><summary>Now compare to answer key</summary>\n\nOpen `docs/TEACHING_ANSWER_KEY.md` -> **Chart 3** and map to memo section `## Top 3 Target Families`.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stop & Answer\n1. Which chart answers retention shape over time by cohort month?  \nAnswer: Chart 1 heatmap (cohort_month x months_since_first).\n\n2. Which chart tells you where to test first-order family interventions?  \nAnswer: Chart 3 M2 retention by `first_product_family` with sample size labels."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "v3-009",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Chart-table previews (same inputs used by story artifact charts).\n",
    "print('Chart1 preview (logo heatmap table):')\n",
    "print(chart1.head(5).to_string(index=False))\n",
    "\n",
    "print('\\nChart2 appendix heatmap preview (net proxy by cohort-month):')\n",
    "print('unique cohorts:', int(chart2['cohort_month'].nunique()))\n",
    "m2 = chart2[chart2['months_since_first'] == 2].copy()\n",
    "print(m2[['cohort_month', 'n_customers', 'net_retention_proxy']].head(10).to_string(index=False))\n",
    "\n",
    "print('\\nChart2 story scatter preview (family impact at M2):')\n",
    "scatter_preview = chart2_scatter[['first_product_family', 'n_customers', 'x_m2_logo_retention', 'y_m2_net_retention_proxy', 'rank_priority']].copy()\n",
    "print(scatter_preview.sort_values('rank_priority', kind='stable').to_string(index=False))\n",
    "\n",
    "print('\\nChart3 table:')\n",
    "print(chart3.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example: Family X\n",
    "Use one family from the scatter to practice executive interpretation from numbers only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked mini-case: choose the weakest net-proxy family among decision-safe n.\n",
    "fam = chart2_scatter.copy()\n",
    "fam['n_customers'] = fam['n_customers'].astype(int)\n",
    "fam = fam[fam['n_customers'] >= MIN_COHORT_N].copy()\n",
    "if fam.empty:\n",
    "    raise ValueError(f'No family rows with n>={MIN_COHORT_N}')\n",
    "\n",
    "case = fam.sort_values(['y_m2_net_retention_proxy', 'rank_priority'], ascending=[True, True], kind='stable').iloc[0]\n",
    "family = str(case['first_product_family'])\n",
    "n_customers = int(case['n_customers'])\n",
    "m2_logo = float(case['x_m2_logo_retention'])\n",
    "m2_net = float(case['y_m2_net_retention_proxy'])\n",
    "\n",
    "print(f'Family: {family}')\n",
    "print(f'n_customers: {n_customers}')\n",
    "print(f'M2 logo retention: {m2_logo:.3f}')\n",
    "print(f'M2 net proxy retention: {m2_net:.3f}')\n",
    "\n",
    "if m2_net < m2_logo:\n",
    "    interpretation = 'Net proxy trails logo -> refund/credit drag is likely part of the weakness.'\n",
    "    action = 'Start with returns-mitigation + post-purchase quality messaging for this family.'\n",
    "else:\n",
    "    interpretation = 'Logo is the main weakness -> repeat demand issue is likely larger than refund drag.'\n",
    "    action = 'Start with replenishment timing + repeat purchase incentive for this family.'\n",
    "\n",
    "print('Interpretation:', interpretation)\n",
    "print('Action:', action)\n",
    "print('Caveat: Directional not causal; validate with controlled experiments.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plain-English: This is the interview move. You name one family, state `n`, compare logo vs net proxy, then pick one concrete action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### You do -> Then we show: Caption writing drill\nWrite your own one-sentence caption for each chart before reading the examples.\n\nYour turn:\n- Chart 1 caption: __________________________\n- Chart 2 caption: __________________________\n- Chart 3 caption: __________________________\n\nStrong example captions:\n- Chart 1: 'Recent cohorts show a similar month-0 start but diverge by months 2-4, indicating uneven repeat decay across acquisition windows.'\n- Chart 2: 'Eligible cohorts separate on net proxy by month 2, showing that refund-adjusted value can diverge even when logo retention looks comparable.'\n- Chart 3: 'Month-2 retention varies materially by first_product_family, with n-labels indicating which family gaps are decision-relevant vs too small to trust.'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "v3-010",
   "source": [
    "## 5) Mini Exercise (runnable)\n",
    "Pick 1 customer and inspect their full 7-row grid.\n",
    "\n",
    "What to explain after running:\n",
    "- Month 0 is cohort start month (first valid purchase month)\n",
    "- Months 1..6 are lifecycle months after first purchase\n",
    "- `is_retained_logo` flips to 1 when there is at least one valid purchase in that month\n",
    "- Censoring check: if `activity_month > max_observed_month`, that month is unobservable and should be missing in right-censored chart tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stop & Answer\n1. Explain `months_since_first` in one sentence.  \nAnswer: It is the integer month distance between an activity month and the customer's cohort month, constrained to 0..6 for this project.\n\n2. Why is month0 retention ~100%?  \nAnswer: Cohort inclusion requires at least one valid purchase, so each included customer has month-0 valid activity by construction."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "v3-011",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Exercise: choose a customer with full grid, print months 0..6, and mark censored rows.\n",
    "exercise_customer_id = str(cma['customer_id'].astype(str).iloc[0])\n",
    "customer_grid = cma[cma['customer_id'].astype(str) == exercise_customer_id].copy()\n",
    "customer_grid = customer_grid.sort_values('months_since_first', kind='stable')\n",
    "\n",
    "max_obs = pd.to_datetime(scope['raw_max_date'], errors='coerce').to_period('M')\n",
    "customer_grid['cohort_period'] = pd.PeriodIndex(customer_grid['cohort_month'].astype(str), freq='M')\n",
    "customer_grid['activity_period'] = customer_grid['cohort_period'] + customer_grid['months_since_first'].astype(int)\n",
    "customer_grid['is_censored_for_charts'] = customer_grid['activity_period'] > max_obs\n",
    "\n",
    "print('exercise_customer_id:', exercise_customer_id)\n",
    "print('max_observed_month:', max_obs)\n",
    "print(customer_grid[['customer_id', 'cohort_month', 'activity_month', 'months_since_first', 'orders_count_valid', 'gross_revenue_valid', 'net_revenue_proxy_total', 'is_retained_logo', 'is_censored_for_charts']].to_string(index=False))\n",
    "\n",
    "censored_months = customer_grid.loc[customer_grid['is_censored_for_charts'], 'months_since_first'].tolist()\n",
    "if censored_months:\n",
    "    print(f'Explanation: months {censored_months} are censored for chart tables because they are beyond max observed month {max_obs}.')\n",
    "else:\n",
    "    print('Explanation: this customer has no censored months within 0..6 given dataset end.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "v3-012",
   "source": "## 6) Expert-style wrap-up (what to say)\n- Grains reconcile from raw to order_lines (no join explosion signal).\n- Cohort construction passes full-grid and Month-0 checks.\n- Gate receipts are in place before interpretation.\n- Three chart questions map directly to one decision: prioritize first-product-family tests.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stop & Answer\n1. What is your one-line decision frame from this analysis?  \nAnswer: Prioritize retention tests by first-order family using M2 logo retention plus net proxy context.\n\n2. Why must you say \"directional, not causal\"?  \nAnswer: The analysis is observational segmentation, not randomized causal inference."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Net Retention Proxy Heatmap (Readable)\n",
    "Appendix-only: this view is cohort-month-centric and mainly for sanity-checking the right-censor/suppression behavior.\n",
    "White/blank = not observed (censored) or suppressed (n<MIN_COHORT_N). Blank is not zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "heat = chart2.copy()\n",
    "heat['months_since_first'] = heat['months_since_first'].astype(int)\n",
    "pivot = heat.pivot(index='cohort_month', columns='months_since_first', values='net_retention_proxy')\n",
    "pivot = pivot.reindex(columns=list(range(HORIZON_H + 1))).sort_index()\n",
    "vals = pivot.values.astype(float)\n",
    "nonnull = vals[~np.isnan(vals)]\n",
    "p95 = float(np.percentile(nonnull, 95)) if len(nonnull) else 1.0\n",
    "vmax = max(1.0, p95)\n",
    "n0 = heat[heat['months_since_first'] == 0][['cohort_month','n_customers_m0']].drop_duplicates()\n",
    "n0_map = dict(zip(n0['cohort_month'].astype(str), n0['n_customers_m0'].astype(int)))\n",
    "\n",
    "mat = np.ma.masked_invalid(vals)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "img = ax.imshow(mat, aspect='auto', cmap='YlGnBu', vmin=0.0, vmax=vmax)\n",
    "ax.set_xticks(range(HORIZON_H + 1))\n",
    "ax.set_xticklabels([str(i) for i in range(HORIZON_H + 1)])\n",
    "yt = pivot.index.astype(str).tolist()\n",
    "ax.set_yticks(range(len(yt)))\n",
    "ax.set_yticklabels([f\"{cm} (n0={n0_map.get(cm,0)})\" for cm in yt])\n",
    "ax.set_xlabel('months_since_first')\n",
    "ax.set_ylabel('cohort_month')\n",
    "ax.set_title(f'Appendix Net Proxy Heatmap (vmax={vmax:.2f}, n>={MIN_COHORT_N})')\n",
    "cbar = fig.colorbar(img, ax=ax)\n",
    "cbar.set_label('net_retention_proxy')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}